{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7fa9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in /Users/syednewaz/anaconda3/lib/python3.10/site-packages (1.1.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/syednewaz/anaconda3/lib/python3.10/site-packages (from scikit-surprise) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/syednewaz/anaconda3/lib/python3.10/site-packages (from scikit-surprise) (1.23.5)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/syednewaz/anaconda3/lib/python3.10/site-packages (from scikit-surprise) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n",
    "#wanted a download as this is a popular tool for recommender systems and I will be using precision@k\n",
    "#and recall@k for my metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bcc273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt\n",
    "import surprise as sp\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096cbb1",
   "metadata": {},
   "source": [
    "I need to bring back some of the code from EDA as there are some specific variables there that will need to be used for the Preprocessing and Model element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e3b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv('cleaned_user_list.csv')\n",
    "anime_df = pd.read_csv('cleaned_anime_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc0ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_anime_df = pd.read_csv('cleaned_user_anime_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1bed1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratings are between: 0 to 10\n"
     ]
    }
   ],
   "source": [
    "ratings_df = user_anime_df[['username','anime_id', 'my_score','my_status']]\n",
    "ratings_df['my_score'].describe().apply(lambda x: format(x, '.2f')).reset_index()\n",
    "lower_rating = ratings_df['my_score'].min()\n",
    "upper_rating = ratings_df['my_score'].max()\n",
    "print(f'The ratings are between: {lower_rating} to {upper_rating}')\n",
    "user_ratings = ratings_df.groupby('username').size().reset_index(name='animes_rated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e001d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_users = user_df.sample(frac = .20, random_state = 2)\n",
    "\n",
    "user_ratings_sampled = pd.merge(\n",
    "    user_ratings, sample_df_users, left_on = 'username', right_on = 'username', how = 'inner')\n",
    "\n",
    "user_ratings_aggregated = user_ratings_sampled.groupby('animes_rated').size().reset_index(\n",
    "    name='group_size').sort_values(by='animes_rated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4fa4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_anime = ratings_df.groupby('anime_id').size().reset_index(name='number_of_users')\n",
    "\n",
    "anime_ratings_aggregated = ratings_anime.groupby('number_of_users').size().reset_index(\n",
    "    name='size_of_users').sort_values(by='number_of_users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed333187",
   "metadata": {},
   "source": [
    "I need to create a method for my metrics before beginning the preprocessing and model, I am going with the Precision@k and Recall@k metric as this is a very common metric for recommender systems and it gives an importance to recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37fcbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def precision_recall(predictions, k=10, threshold=7):\n",
    "    \n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    user_metrics = []\n",
    "\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        precision = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        recall = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "        user_metrics.append({'uid': uid, 'precision': precision, 'recall': recall})\n",
    "\n",
    "    user_metrics = pd.DataFrame(user_metrics)\n",
    "\n",
    "    return user_metrics\n",
    "'''\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold= 7):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    # Predictions: Traz uma lista de 5 campos dentro de uma tupla com as seguintes infos: User_ID, Item_ID, True_ID, Est_ID, Details\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    # Creates a dict with the key being a user and the value bringing the estimated rating and the true rating.\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k /  n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d47aad",
   "metadata": {},
   "source": [
    "We might run into this issue called a \"cold start\" which essentially is a problem for users that do not have historical data for us to go off of and recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2fd5bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_10 = user_ratings_sampled[user_ratings_sampled['animes_rated'] >= 10]\n",
    "anime_ratings_10 = ratings_anime[ratings_anime['number_of_users'] >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "769df90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_start = pd.merge(pd.merge(ratings_df, user_ratings_10, on='username', how='inner'), \n",
    "                            anime_ratings_10, on='anime_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d0bd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial dataframe has 80075851 registers and the sampled one has 16051272 rows.\n"
     ]
    }
   ],
   "source": [
    "print('The initial dataframe has {0} registers and the sampled one has {1} rows.'.format(\n",
    "    ratings_df['username'].count(), rating_start['username'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72345603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>my_score</th>\n",
       "      <th>my_status</th>\n",
       "      <th>animes_rated</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_watching</th>\n",
       "      <th>user_completed</th>\n",
       "      <th>user_onhold</th>\n",
       "      <th>user_dropped</th>\n",
       "      <th>user_plantowatch</th>\n",
       "      <th>user_days_spent_watching</th>\n",
       "      <th>join_date</th>\n",
       "      <th>last_online</th>\n",
       "      <th>stats_mean_score</th>\n",
       "      <th>stats_rewatched</th>\n",
       "      <th>stats_episodes</th>\n",
       "      <th>number_of_users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>karthiga</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>2255153</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.31</td>\n",
       "      <td>2013-03-03</td>\n",
       "      <td>2014-02-04 01:32:00</td>\n",
       "      <td>7.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3391.0</td>\n",
       "      <td>106378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Slimak</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>390</td>\n",
       "      <td>61677</td>\n",
       "      <td>79</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>84</td>\n",
       "      <td>126.17</td>\n",
       "      <td>2008-05-18</td>\n",
       "      <td>1900-05-01 05:04:00</td>\n",
       "      <td>7.77</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7472.0</td>\n",
       "      <td>106378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MistButterfly</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4472</td>\n",
       "      <td>2485327</td>\n",
       "      <td>66</td>\n",
       "      <td>3923</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>368</td>\n",
       "      <td>614.96</td>\n",
       "      <td>2013-04-25</td>\n",
       "      <td>2018-05-17 13:31:00</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39893.0</td>\n",
       "      <td>106378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cfoordddd</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>716</td>\n",
       "      <td>61291</td>\n",
       "      <td>5</td>\n",
       "      <td>392</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>288</td>\n",
       "      <td>151.61</td>\n",
       "      <td>2008-05-17</td>\n",
       "      <td>1900-04-18 00:58:00</td>\n",
       "      <td>7.94</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10027.0</td>\n",
       "      <td>106378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xTheFallenx</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>894</td>\n",
       "      <td>340873</td>\n",
       "      <td>22</td>\n",
       "      <td>655</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>212</td>\n",
       "      <td>131.89</td>\n",
       "      <td>2010-06-13</td>\n",
       "      <td>1900-05-16 03:20:00</td>\n",
       "      <td>8.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8508.0</td>\n",
       "      <td>106378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        username  anime_id  my_score  my_status  animes_rated  user_id  \\\n",
       "0       karthiga        21         9          1            53  2255153   \n",
       "1         Slimak        21        10          1           390    61677   \n",
       "2  MistButterfly        21         0          1          4472  2485327   \n",
       "3      cfoordddd        21        10          1           716    61291   \n",
       "4    xTheFallenx        21         7          1           894   340873   \n",
       "\n",
       "   user_watching  user_completed  user_onhold  user_dropped  user_plantowatch  \\\n",
       "0              3              49            1             0                 0   \n",
       "1             79             224            0             3                84   \n",
       "2             66            3923          115             0               368   \n",
       "3              5             392           31             0               288   \n",
       "4             22             655            2             3               212   \n",
       "\n",
       "   user_days_spent_watching   join_date          last_online  \\\n",
       "0                     55.31  2013-03-03  2014-02-04 01:32:00   \n",
       "1                    126.17  2008-05-18  1900-05-01 05:04:00   \n",
       "2                    614.96  2013-04-25  2018-05-17 13:31:00   \n",
       "3                    151.61  2008-05-17  1900-04-18 00:58:00   \n",
       "4                    131.89  2010-06-13  1900-05-16 03:20:00   \n",
       "\n",
       "   stats_mean_score  stats_rewatched  stats_episodes  number_of_users  \n",
       "0              7.43              0.0          3391.0           106378  \n",
       "1              7.77              2.0          7472.0           106378  \n",
       "2              5.49              0.0         39893.0           106378  \n",
       "3              7.94              6.0         10027.0           106378  \n",
       "4              8.02              0.0          8508.0           106378  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_start.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04915b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratings_start still has too much, we will need to sample this even further. I'm going reduce this to around 500,000 rows\n",
    "random_state = 42\n",
    "sample_fraction = 0.04\n",
    "sample_df = rating_start.sample(frac=sample_fraction, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ade69bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial dataframe has 80075851 registers and the sampled one has 642051 rows.\n"
     ]
    }
   ],
   "source": [
    "print('The initial dataframe has {0} registers and the sampled one has {1} rows.'.format(\n",
    "    ratings_df['username'].count(), sample_df['username'].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ed65d",
   "metadata": {},
   "source": [
    "# Training and PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e09e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = sp.Reader(rating_scale=(0, 10))\n",
    "data = sp.Dataset.load_from_df(sample_df[['username', 'anime_id', 'my_score']], reader)\n",
    "trainset, testset = sp.model_selection.train_test_split(data, test_size=.25, random_state = random_state)\n",
    "analysis = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85b5168f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"reader = sp.Reader(rating_scale=(0, 10))\\ndata = sp.Dataset.load_from_df(sample_df[['username', 'anime_id', 'my_score']], reader)\\ntrainset, testset = sp.model_selection.train_test_split(data, test_size=.25, random_state=random_state)\\nanalysis = pd.DataFrame()\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''reader = sp.Reader(rating_scale=(0, 10))\n",
    "data = sp.Dataset.load_from_df(sample_df[['username', 'anime_id', 'my_score']], reader)\n",
    "trainset, testset = sp.model_selection.train_test_split(data, test_size=.25, random_state=random_state)\n",
    "analysis = pd.DataFrame()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28690bf4",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c4e5d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {'SVD' : sp.SVD(random_state=random_state), \n",
    "             'SlopeOne' : sp.SlopeOne(), \n",
    "             'NMF' : sp.NMF(random_state=random_state),\n",
    "             'BaselineOnly' : sp.BaselineOnly(),\n",
    "             'NormalPredictor' : sp.NormalPredictor(), \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b70911a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training <surprise.prediction_algorithms.matrix_factorization.SVD object at 0x7faed70de950>\n",
      "RMSE: 3.5072\n",
      "Training <surprise.prediction_algorithms.slope_one.SlopeOne object at 0x7faed70dc370>\n",
      "RMSE: 3.5296\n",
      "Training <surprise.prediction_algorithms.matrix_factorization.NMF object at 0x7faed70dc0a0>\n",
      "RMSE: 3.7779\n",
      "Training <surprise.prediction_algorithms.baseline_only.BaselineOnly object at 0x7faed70dc0d0>\n",
      "Estimating biases using als...\n",
      "RMSE: 3.3672\n",
      "Training <surprise.prediction_algorithms.random_pred.NormalPredictor object at 0x7faed70dcca0>\n",
      "RMSE: 5.0783\n",
      "defaultdict(<class 'list'>, {<surprise.prediction_algorithms.matrix_factorization.SVD object at 0x7faed70de950>: ('SVD', 3.50718738687803, 0.9485712686288662, 10.297825813293457), <surprise.prediction_algorithms.slope_one.SlopeOne object at 0x7faed70dc370>: ('SlopeOne', 3.529564882302046, 0.8959691907182773, 13.145518064498901), <surprise.prediction_algorithms.matrix_factorization.NMF object at 0x7faed70dc0a0>: ('NMF', 3.7778662718659652, 0.8549110166462685, 14.75487208366394), <surprise.prediction_algorithms.baseline_only.BaselineOnly object at 0x7faed70dc0d0>: ('BaselineOnly', 3.3671570168289224, 0.9911739843351073, 2.691204071044922), <surprise.prediction_algorithms.random_pred.NormalPredictor object at 0x7faed70dcca0>: ('NormalPredictor', 5.078287702464967, 0.6970548650380972, 1.51619291305542)})\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for key, value in test_dict.items():\n",
    "        print(f\"Training {value}\")\n",
    "        start = time.time()    \n",
    "        value.fit(trainset)\n",
    "        predictions = value.test(testset)\n",
    "\n",
    "        rmse = sp.accuracy.rmse(predictions)\n",
    "        precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=7)\n",
    "        precision_avg = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "\n",
    "        analysis[value] = (key, rmse, precision_avg, time.time() - start)\n",
    "\n",
    "    print(analysis)\n",
    "except Exception as e:\n",
    "    print(f\"Error while training {value}\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf16013",
   "metadata": {},
   "source": [
    "I initially had KNN models, however it was causing my kernel to die, and lose memory. I made the decision to not use any of the KNN models, and will only be analyzing the ones above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d160da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame.from_dict(analysis, orient = 'index', \n",
    "                                     columns = ['Algorithm', 'RMSE', 'Precision@10', 'Time to run (in seconds)']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "620c93f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Time to run (in seconds)</th>\n",
       "      <th>RMSE^-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BaselineOnly</td>\n",
       "      <td>3.367157</td>\n",
       "      <td>0.991174</td>\n",
       "      <td>2.691204</td>\n",
       "      <td>0.296986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVD</td>\n",
       "      <td>3.507187</td>\n",
       "      <td>0.948571</td>\n",
       "      <td>10.297826</td>\n",
       "      <td>0.285129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SlopeOne</td>\n",
       "      <td>3.529565</td>\n",
       "      <td>0.895969</td>\n",
       "      <td>13.145518</td>\n",
       "      <td>0.283321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NMF</td>\n",
       "      <td>3.777866</td>\n",
       "      <td>0.854911</td>\n",
       "      <td>14.754872</td>\n",
       "      <td>0.264700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NormalPredictor</td>\n",
       "      <td>5.078288</td>\n",
       "      <td>0.697055</td>\n",
       "      <td>1.516193</td>\n",
       "      <td>0.196917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Algorithm      RMSE  Precision@10  Time to run (in seconds)   RMSE^-1\n",
       "3     BaselineOnly  3.367157      0.991174                  2.691204  0.296986\n",
       "0              SVD  3.507187      0.948571                 10.297826  0.285129\n",
       "1         SlopeOne  3.529565      0.895969                 13.145518  0.283321\n",
       "2              NMF  3.777866      0.854911                 14.754872  0.264700\n",
       "4  NormalPredictor  5.078288      0.697055                  1.516193  0.196917"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df = analysis_df[['Algorithm', 'RMSE', 'Precision@10', 'Time to run (in seconds)']]\n",
    "analysis_df = analysis_df.sort_values(by=['Precision@10'], ascending = False)\n",
    "analysis_df['RMSE^-1'] = analysis_df['RMSE'] ** -1\n",
    "analysis_df.head(n = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a5334",
   "metadata": {},
   "source": [
    "Based on the models above, it looks like the best performing model is BaselineOnly so i will be using this model. it is the simplest and the least costing collaborative filtering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d10b92f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "als_param_grid = {'bsl_options': {'method': ['als'],\n",
    "                              'reg_i': [5, 10, 15],\n",
    "                              'reg_u': [10, 15, 20],\n",
    "                              'n_epochs': [5, 10, 15, 20]\n",
    "                              }\n",
    "              }\n",
    "\n",
    "sgd_param_grid = {'bsl_options': {'method': ['sgd'],\n",
    "                              'reg': [0.01, 0.02, 0.03],\n",
    "                              'n_epochs': [5, 10, 15, 20],\n",
    "                              'learning_rate' : [0.001, 0.005, 0.01]\n",
    "                              }\n",
    "              }\n",
    "\n",
    "als_gs = sp.model_selection.GridSearchCV(sp.BaselineOnly, als_param_grid, measures=['rmse'], cv = 3, joblib_verbose = 0)\n",
    "\n",
    "sgd_gs = sp.model_selection.GridSearchCV(sp.BaselineOnly, sgd_param_grid, measures=['rmse'], cv = 3, joblib_verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b640121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "3.3443333895419123\n",
      "{'bsl_options': {'method': 'als', 'reg_i': 5, 'reg_u': 10, 'n_epochs': 20}}\n"
     ]
    }
   ],
   "source": [
    "als_gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(als_gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(als_gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cfc241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "3.278132192065234\n",
      "{'bsl_options': {'method': 'sgd', 'reg': 0.02, 'n_epochs': 20, 'learning_rate': 0.01}}\n"
     ]
    }
   ],
   "source": [
    "sgd_gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(sgd_gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(sgd_gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2010036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n"
     ]
    }
   ],
   "source": [
    "trainset = data.build_full_trainset()\n",
    "algo = sp.BaselineOnly()\n",
    "algo.fit(trainset)\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "    \n",
    "last_predictions = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
    "last_predictions.drop('rui', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880500f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
